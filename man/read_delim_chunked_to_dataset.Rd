% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/arrow.R
\name{read_delim_chunked_to_dataset}
\alias{read_delim_chunked_to_dataset}
\alias{write_single_partition_dataset}
\title{Read a delimited file by chunks and write into Hive-style Parquet files}
\usage{
read_delim_chunked_to_dataset(
  file,
  dataset_base_name,
  file_nrow,
  chunk_size,
  processing_function = NULL,
  chunk_col_name = "chunk",
  chunk_file_name = "data.parquet",
  ...
)

write_single_partition_dataset(
  df,
  dataset_base_name,
  chunk_col_name = "chunk",
  chunk_file_name = "data.parquet"
)
}
\arguments{
\item{file}{Either a path to a file, a connection, or literal data
(either a single string or a raw vector).

Files ending in \code{.gz}, \code{.bz2}, \code{.xz}, or \code{.zip} will
be automatically uncompressed. Files starting with \verb{http://},
\verb{https://}, \verb{ftp://}, or \verb{ftps://} will be automatically
downloaded. Remote gz files can also be automatically downloaded and
decompressed.

Literal data is most useful for examples and tests. It must contain at
least one new line to be recognised as data (instead of a path) or be a
vector of greater than length 1.

Using a value of \code{\link[readr:clipboard]{clipboard()}} will read from the system clipboard.}

\item{dataset_base_name}{Path of the directory to write the Hive
partitioned Parquet files to.}

\item{file_nrow}{Number of data rows in \code{file}. As there is no
reliable and cross-platform way to get the exact number of lines
in a compressed file, this has to be set manually to calculate
the number of chunks and the names of partitions. Use \code{wc}
on a Unix-like system to determine row count (\code{zcat file.gz
| wc -l}, or similar). Only count rows considered as data,
otherwise the dataset's partitioning scheme will have empty
directories. This does not result in errors but it is undesirable
for human-readability. Subtract from the row count any header
row(s), or the number of lines skipped with the \code{skip}
(again, \code{zcat file.gz | head}, or similar can be useful).}

\item{chunk_size}{The number of rows to include in each chunk}

\item{processing_function}{A function that takes each chunk and
does arbitrary data processing on it before writing the resulting
data frame into its Parquet partition.}

\item{chunk_col_name}{Name of the column indicating partition
numbers in the Hive-style partition structure.}

\item{chunk_file_name}{Name of the individual Parquet files in the
Hive-style partition structure.}

\item{...}{Passed to \code{\link[readr]{read_delim_chunked}}}

\item{df}{A data frame}
}
\value{
Invisibly return a tibble with parsing problems caught by
  \pkg{readr} (see \code{\link[readr]{problems}}). \code{NULL} if
  no parsing problems occurred.
}
\description{
Read a single delimited file in chunks using
\code{\link[readr]{read_delim_chunked}} and save chunks in Parquet
files under a simple Hive-style partitioned directory (i.e.
\code{dataset_base_name/chunk=XX/data.parquet}) to be used as the
source of a multi-file Apache Arrow dataset.
}
\details{
The main goal of this function is to read a single, large,
  unpartitioned delimited file into a partitioned Arrow dataset on
  a RAM limited machine. Therefore these Arrow partitions have no
  inherent meaning. Although \code{processing_function} allows
  flexible changes during reading in, this function was intended to
  be used in workflows where only minimal data processing is done
  and the original structure of the delimited files is kept
  unchanged. Thus \code{read_delim_chunked_to_dataset} will create
  a partitioning that keeps the original row order from the
  delimited file. However, within partition ordering can be changed
  through \code{processing_function}.
}
\section{Functions}{
\itemize{
\item \code{write_single_partition_dataset}: 
}}

\seealso{
\code{vignette(topic = "dataset", package = "arrow")} on
how to use mult-file Apache Arrow datasets.
}
