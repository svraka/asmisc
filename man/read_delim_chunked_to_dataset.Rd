% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/arrow.R
\name{read_delim_chunked_to_dataset}
\alias{read_delim_chunked_to_dataset}
\title{Read a delimited file by chunks and write into Hive-style Parquet files}
\usage{
read_delim_chunked_to_dataset(
  file,
  dataset_base_name,
  file_nrow,
  chunk_size,
  processing_function = NULL,
  chunk_col_name = "chunk",
  chunk_file_name = "data.parquet",
  ...
)
}
\arguments{
\item{file}{Either a path to a file, a connection, or literal data
(either a single string or a raw vector).

Files ending in \code{.gz}, \code{.bz2}, \code{.xz}, or \code{.zip} will
be automatically uncompressed. Files starting with \code{http://},
\code{https://}, \code{ftp://}, or \code{ftps://} will be automatically
downloaded. Remote gz files can also be automatically downloaded and
decompressed.

Literal data is most useful for examples and tests. It must contain at
least one new line to be recognised as data (instead of a path) or be a
vector of greater than length 1.

Using a value of \code{\link[readr:clipboard]{clipboard()}} will read from the system clipboard.}

\item{dataset_base_name}{Path of the directory to write the Hive
partitioned Parquet files to.}

\item{file_nrow}{Number of rows in \code{file}. As there is no
reliable and cross-platform way to get the exact number of lines
in a compressed file, this has to be set manually to calculate
the number of chunks and the names of partitions. Use \code{wc}
on a Unix-like system to determine row count (\code{zcat file.gz
| wc -l}, or similar).}

\item{chunk_size}{The number of rows to include in each chunk}

\item{processing_function}{A function that takes each chunk and
does arbitrary data processing on it before writing it into its
Parquet partition.}

\item{chunk_col_name}{Name of the column indicating partition
numbers in the Hive-style partition structure.}

\item{chunk_file_name}{Name of the individual Parquet files in the
Hive-style partition structure.}

\item{...}{Passed to \code{\link[readr]{read_delim_chunked}}}
}
\value{
Invisibly return a tibble with parsing problems caught by
  \pkg{readr} (see \code{\link[readr]{problems}}). \code{NULL} if
  no parsing problems occurred.
}
\description{
Read a single delimited file in chunks using
\code{\link[readr]{read_delim_chunked}} and save chunks in Parquet
files under a simple Hive-style partitioned directory (i.e.
\code{dataset_base_name/chunk=XX/data.parquet}) to be used as the
source of a multi-file Apache Arrow dataset.
}
\details{
The main goal of this function is to read a single, large,
  unpartitioned delimited file into an Arrow dataset on a RAM
  limited machine.
}
\seealso{
\code{vignette(topic = "dataset", package = "arrow")} on
how to use mult-file Apache Arrow datasets.
}
